{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO500 Results Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import datetime\n",
    "import configparser\n",
    "\n",
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_BASE = '/global/homes/g/glock/src/git/io500/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_io500_results(result_dir):\n",
    "    \"\"\"Converts a directory of IO500 results into a dictionary\n",
    "    \n",
    "    Args:\n",
    "        result_dir (str): Path to a directory containing the results of a single IO500 run\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains results of the form::\n",
    "        \n",
    "            {\n",
    "                \"job\": {\"api\", \"clients per node\", \"nodes\", \"tasks\", \"xfersize\"},\n",
    "                \"result\": {\"DEFAULT\", \"SCORE\", \"find\", \"header\", \"ior-easy-read\", ...}\n",
    "            }\n",
    "    \"\"\"\n",
    "    RESULT_CONVERTS = {\n",
    "        \"bw\": float,\n",
    "        \"md\": float,\n",
    "        \"score\": float,\n",
    "        \"t_delta\": float,\n",
    "        \"rate-stonewall\": float,\n",
    "        \"throughput-stonewall\": float,\n",
    "        \"total-files\": int,\n",
    "        \"found\": int,\n",
    "        \"t_end\": lambda x: int(datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").timestamp()),\n",
    "        \"t_start\": lambda x: int(datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").timestamp()),\n",
    "    }\n",
    "    results = {}\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    result_file = os.path.join(result_dir, 'result.txt')\n",
    "    if not os.path.isfile(result_file):\n",
    "        return results\n",
    "\n",
    "    # parse result file\n",
    "    result_text = open(result_file, 'r').read()\n",
    "    # result file has some keys lacking a header at the top; unsure why...\n",
    "    result_text = \"[header]\\n\" + result_text\n",
    "    config.read_string(result_text)\n",
    "    results['result'] = {}\n",
    "    for section, items in config.items():\n",
    "        results['result'][section] = {k: float(v) if k == 'score' else v for k, v in items.items()}\n",
    "    # clean up types\n",
    "    for testname, testresults in results['result'].items():\n",
    "        for key, value in testresults.items():\n",
    "            testresults[key] = RESULT_CONVERTS.get(key, str)(value)\n",
    "    \n",
    "    # parse ior for job geometry\n",
    "    fp = open(os.path.join(result_dir, 'ior-easy-write.txt'), 'r')\n",
    "    results['job'] = {}\n",
    "    for line in fp:\n",
    "        if ':' in line:\n",
    "            key, val = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            val = val.strip()\n",
    "        if key in ('nodes', 'tasks', 'clients per node'):\n",
    "            results['job'][key] = int(val)\n",
    "        elif key in ('api',):\n",
    "            results['job'][key] = val\n",
    "        elif key in ('xfersize',):\n",
    "            if val.endswith('KiB'):\n",
    "                results['job'][key] = int(val.split()[0]) * 2**10\n",
    "            elif val.endswith('MiB'):\n",
    "                results['job'][key] = int(val.split()[0]) * 2**20\n",
    "            elif val.endswith('GiB'):\n",
    "                results['job'][key] = int(val.split()[0]) * 2**30\n",
    "            elif val.endswith('bytes'):\n",
    "                results['job'][key] = int(val.split()[0])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Load all result directories found in the path given by `RESULTS_BASE` and construct a dataframe containing summary metrics from each IO500 run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for result_dir in glob.glob(os.path.join(RESULTS_BASE, '*')):\n",
    "    result = parse_io500_results(result_dir)\n",
    "    records.append({\n",
    "        'result_id': os.path.basename(result_dir),\n",
    "        'score': result.get('result', {}).get('SCORE', {}).get(\"score\", -1),\n",
    "        'bw': result.get('result', {}).get('SCORE', {}).get(\"bw\", -1),\n",
    "        'md': result.get('result', {}).get('SCORE', {}).get(\"md\", -1),\n",
    "        'clients per node': result.get('job', {}).get('clients per node', -1),\n",
    "        'nodes': result.get('job', {}).get('nodes', -1),\n",
    "        'xfersize': result.get('job', {}).get('xfersize', -1),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pandas.DataFrame.from_records(records, index='result_id').sort_values('clients per node')\n",
    "results_df = results_df[results_df['score'] > 0.0]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select runs to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XFERSIZE = 16*8388608\n",
    "COLUMN_TITLES = {\n",
    "    \"bw\": \"Bandwidth\",\n",
    "    \"md\": \"Metadata\",\n",
    "    \"score\": \"Score\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = results_df['xfersize'] == XFERSIZE\n",
    "filt = results_df['xfersize'].astype(bool)\n",
    "plot_df = results_df[filt]\n",
    "plot_df = plot_df.set_index('clients per node')\n",
    "plot_df.columns = [COLUMN_TITLES.get(x, x) for x in plot_df.columns]\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot IO500 Score Components\n",
    "\n",
    "Plot both the bandwidth and metadata scores to see how they compare to the singular \"score\" that pops out at the end of IO500 as different tuning parameters change.  This plot shows that metadata and bandwidth performance can no only diverge wildly, but they may show anticorrelative behavior with increasing concurrency.\n",
    "\n",
    "This indicates that the highest IO500 score isn't necessarily the best bandwidth or best metadata scores; rather, it's an arbitrary point of balance between increasing concurrency yielding increasing bandwidth and too much concurrency conflicting with metadata operations.\n",
    "\n",
    "It is easy to envision a way to optimally design a file system that scores high on IO-500; since metadata scores are weighted more highly than bandwidth by virtue of the fact that they are measured in kIOPS vs. GiB/s, a good IO500 score\n",
    "\n",
    "1. posts the best possible metadata scores, probably by scaling wide and minimizing the amount of metadata contention that each client generates\n",
    "2. posts a good ior-easy\n",
    "3. doesn't matter what ior-hard scores, because the first two will drag the score up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "\n",
    "plot_df[['Bandwidth', 'Metadata']].plot.bar(ax=ax, width=0.9, edgecolor='black', legend=False)\n",
    "for i in range(plot_df.shape[0]):\n",
    "    ax.plot((i-0.35, i+0.35),\n",
    "            (plot_df['Score'].iloc[i], plot_df['Score'].iloc[i]),\n",
    "            linestyle='-',\n",
    "            linewidth=4,\n",
    "#           marker='D',\n",
    "#           markersize=9,\n",
    "            color='C2',\n",
    "            label=\"Score\" if not i else None\n",
    "           )\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid()\n",
    "ax.set_title(\"10-node IO500 scores - DataWarp\")\n",
    "ax.legend(\n",
    "    bbox_to_anchor=(0.5, -0.40, 0, 0),\n",
    "    loc='lower center',\n",
    "#   mode='expand',\n",
    "#   borderaxespad=0.0,\n",
    "    ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the range of bw/md scores that a benchmark can post to achieve the same overall score.  There are two orders of magnitude in variation for the metadata scores and three orders of magnitude change for bandwidth scores as a result of the imbalance of kIOPS and GiB/s.  This is because\n",
    "\n",
    "${md} = \\frac{score^2}{bw}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DW_MAX_BW_10NODE = 80.0 # GiB/s\n",
    "DW_MAX_MD_10NODE = 80.0 # KIOPS\n",
    "DW_MAX_SCORE_10NODE = plot_df['Score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = matplotlib.pyplot.subplots()\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"kIOPS\")\n",
    "ax.set_xlabel(\"GiB/s\")\n",
    "\n",
    "for target_score in (DW_MAX_SCORE_10NODE, ):\n",
    "    x = numpy.arange(1, DW_MAX_BW_10NODE)\n",
    "    y = target_score*target_score / x\n",
    "\n",
    "    ax.plot(x, y, label=\"Score = %.1f\" % target_score, color=\"black\")\n",
    "ax.legend()\n",
    "\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xlim(0, DW_MAX_BW_10NODE)\n",
    "ax.set_ylim(0, DW_MAX_MD_10NODE)\n",
    "ax.set_title(\"Iso score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that in order to maintain a constant score,\n",
    "\n",
    "1. if bandwidths are very low, significantly higher IOPS are required\n",
    "2. if bandwidths are very high, metadata improvements are small\n",
    "\n",
    "I guess the above statements are trivial.  Where does a real file system land on this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_opts = {\n",
    "    'marker': 'o',\n",
    "#   'markersize': 9,\n",
    "}\n",
    "color_map = {}\n",
    "legend_handles, legend_labels = ax.get_legend_handles_labels()\n",
    "for idx, val in enumerate(plot_df['xfersize'].unique()):\n",
    "    color_map[val] = 'C%d' % idx\n",
    "    legend_handles.append(\n",
    "        matplotlib.lines.Line2D(\n",
    "            [], [],\n",
    "            linestyle=\"\",\n",
    "            color=color_map[val],\n",
    "            **marker_opts))\n",
    "    legend_labels.append(\n",
    "        \"blksize=%dK\" % (val / 2**10) if val < 2**20 else \"blksize=%dM\" % (val / 2**20)\n",
    "    )\n",
    "\n",
    "x = plot_df['Bandwidth'].values\n",
    "y = plot_df['Metadata'].values\n",
    "ax.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    color=plot_df['xfersize'].map(lambda x: color_map.get(x)),\n",
    "    **marker_opts)\n",
    "\n",
    "for i, txt in enumerate(plot_df.index.values):\n",
    "    ax.annotate(txt, (x[i]+1.0, y[i]+1.0))\n",
    "ax.legend(legend_handles, legend_labels)\n",
    "ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On DataWarp, metadata scores are much less volatile than bandwidth scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
